{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8522dbbb",
   "metadata": {},
   "source": [
    "#Esta es la versión estocastoca del problema del robot limpiador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a3b61a3d",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2231656779.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[62], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    El aprendizaje por refuerzo (RL), los modelos **model-based** utilizan una representación explícita de la dinámica del entorno para planificar y tomar decisiones. Cuando el entorno es **estocástico**, las transiciones entre estados no son deterministas, sino que están gobernadas por probabilidades.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Resumen de Modelos de RL Model-Based Estocásticos\n",
    "El aprendizaje por refuerzo (RL), los modelos **model-based** utilizan una representación explícita de la dinámica del entorno para planificar y tomar decisiones. Cuando el entorno es **estocástico**, las transiciones entre estados no son deterministas, sino que están gobernadas por probabilidades.\n",
    "\n",
    "### Características principales\n",
    "\n",
    "- **Modelo de transición**: Define la probabilidad de pasar de un estado a otro dado una acción, es decir, \\( P(s'|s, a) \\).\n",
    "- **Modelo de recompensa**: Especifica la recompensa esperada al tomar una acción en un estado, \\( R(s, a, s') \\).\n",
    "- **Planificación**: Se utilizan algoritmos como Value Iteration o Policy Iteration para calcular la política óptima usando el modelo aprendido o conocido.\n",
    "\n",
    "### Ejemplos de algoritmos\n",
    "\n",
    "- **Value Iteration Estocástico**: Actualiza los valores de los estados considerando la expectativa sobre todas las posibles transiciones.\n",
    "- **Policy Iteration Estocástico**: Alterna entre evaluar la política actual y mejorarla, considerando la naturaleza probabilística de las transiciones.\n",
    "- **Q-Iteration Estocástico**: Extiende la iteración Q al caso donde las transiciones y recompensas son probabilísticas.\n",
    "\n",
    "### Ventajas\n",
    "\n",
    "- Permiten planificar a futuro considerando la incertidumbre del entorno.\n",
    "- Pueden adaptarse a cambios en la dinámica si el modelo se aprende en línea.\n",
    "\n",
    "### Desventajas\n",
    "\n",
    "- Requieren conocer o aprender el modelo de transición y recompensa.\n",
    "- El cálculo puede ser costoso en entornos con muchos estados o acciones.\n",
    "\n",
    "### Aplicaciones\n",
    "\n",
    "- Robótica móvil en ambientes inciertos.\n",
    "- Juegos y simulaciones donde las acciones tienen resultados probabilísticos.\n",
    "- Toma de decisiones en sistemas dinámicos complejos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b029ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class enviroment():\n",
    "    def __init__(self):\n",
    "        \"\"\"Esta clase cre el ambiente del robot movil \n",
    "\n",
    "        * self.numeroDeEstados -> Numero de estados\n",
    "        * self.acciones -> Acciones permitidas por el robot\n",
    "        * self.numeroDeAcciones -> Numero de acciones permitidas por el robot \n",
    "\n",
    "        \"\"\"\n",
    "        self.numeroDeEstados=6 #Numero de estados para el problema del robot\n",
    "        self.acciones=[-1,1] #Accciones del problema del robot\n",
    "        self.numeroDeAcciones=len(self.acciones) #Numero de acciones\n",
    "        self.cambios = [1, 0, -1]  # S+1, S+0, S-1\n",
    "        self.probabilidades = [0.8, 0.15, 0.05]  # 80%, 5%, 15%\n",
    "        self.Dinamica_delrobot_limpiador_estocastico=[[1.00,0.00,0.00,0.00,0.00,0.00],\n",
    "                                                      [0.80,0.15,0.05,0.00,0.00,0.00],\n",
    "                                                      [0.00,0.80,0.15,0.05,0.00,0.00],\n",
    "                                                      [0.00,0.00,0.80,0.15,0.05,0.00],\n",
    "                                                      [0.00,0.00,0.00,0.80,0.15,0.05],\n",
    "                                                      [0.00,0.00,0.00,0.00,0.00,1.00],\n",
    "                                                      [1.00,0.00,0.00,0.00,0.00,0.00],\n",
    "                                                      [0.05,0.15,0.80,0.00,0.00,0.00],\n",
    "                                                      [0.00,0.05,0.15,0.80,0.00,0.00],\n",
    "                                                      [0.00,0.00,0.05,0.15,0.80,0.00],\n",
    "                                                      [0.00,0.00,0.00,0.05,0.15,0.80],\n",
    "                                                      [0.00,0.00,0.00,0.00,0.00,1.00]]\n",
    "    def reset(self):\n",
    "        \"\"\"Esta función resetea el robot a la posición inicial, se recomienda utilizar justo depues de inicializar el ambiente \n",
    "            ejemplo enviroment=enviroment() --> inicia el ambiente\n",
    "        *self.current_state: 3, Estado actual del robot, al utiizar reset el estado del robot regresa a 3 independientemente de donde este\n",
    "                                por ejemplo enviroment.reset() --> resetea al estado 3\n",
    "        * terminado: False, indica que un episodio ha terminado\n",
    "        * truncate: False, indica que el agente ha llegado a un punto en donde no puede avanzar y hay que terminar el estado el espisodio \n",
    "        \"\"\"\n",
    "        self.current_state=3\n",
    "        recompensa=0\n",
    "        self.terminado=False\n",
    "        self.Truncado=False\n",
    "        self.informacion=\"El robot ha empezado\"\n",
    "        return self.current_state, recompensa, self.terminado, self.Truncado \n",
    "    def step(self,action: int):\n",
    "        \"\"\"Este metodo contiene las dinamicas del ambiente\n",
    "\n",
    "        Args:\n",
    "            action (int): contiene la acción del robot, puede ser -1 o 1\n",
    "\n",
    "        Returns:\n",
    "            _int, float, bool, bool_: Estado actual, recompensa, si termino, o quedo estancado \n",
    "        \"\"\"\n",
    "        #RECOMPENSA------------------------------------------\n",
    "        recompensa=self.reward(action)\n",
    "        #----------------------------------------------------\n",
    "        #CAMBIO DE ESTADO------------------------------------\n",
    "        if 1<=self.current_state  and self.current_state<=4:\n",
    "            #Implementación elegante de la aleatoriedad\n",
    "            cambio = random.choices(self.cambios, weights=self.probabilidades)[0]\n",
    "            #----------------------------------------\n",
    "            self.current_state+=action#*cambio\n",
    "        return self.current_state, recompensa, self.terminado, self.Truncado \n",
    "\n",
    "    def reward(self,stocastic_state):\n",
    "        \"\"\"Genera la recompensa del agente al tomar la acción a_i en un estado s_i\n",
    "\n",
    "        Args:\n",
    "            action (_lista_): [-1,1] \n",
    "\n",
    "        Returns:\n",
    "            _float_: _retorna la recomepensa_\n",
    "        \"\"\"\n",
    "        #RECOMPENSA DETERMINISTICA------------------------------------------\n",
    "        \"\"\"recompensa=0\n",
    "        if self.current_state==4 and action==1:\n",
    "            recompensa=5\n",
    "            self.terminado=True\n",
    "        if self.current_state==1 and action==-1:\n",
    "            recompensa=1\n",
    "            self.Truncado=True \"\"\"\n",
    "        #----------------------------------------------------\n",
    "        #RECOMPENSA ESTOCASTICA------------------------------\n",
    "        recompensa=0\n",
    "        if self.current_state!=5 and stocastic_state==5:\n",
    "            recompensa=5\n",
    "            self.terminado=True\n",
    "        if self.current_state!=0 and stocastic_state==0:\n",
    "            recompensa=1\n",
    "            self.Truncado=True\n",
    "        return recompensa\n",
    "\n",
    "class Qiteration():\n",
    "    \"\"\"_Se encarga de realizar la iteración Q para el caso deterministico del robot limpiador_\n",
    "    \"\"\"\n",
    "    def __init__(self,env,numeroDeEstados: int,numeroDeAcciones: int):\n",
    "        \"\"\"_Inicializa la iteración Q_\n",
    "\n",
    "        Args:\n",
    "            env (_class_): _Ambiente con las dinamicas de Transición _\n",
    "            numeroDeEstados (int): _Numero de estados_\n",
    "            numeroDeAcciones (int): _Numero de acciones_\n",
    "        \"\"\"\n",
    "        self.env=env\n",
    "        self.matriz_estado_accion=np.zeros((numeroDeEstados,numeroDeAcciones))\n",
    "        self.matriz_estado_accion_estocastico=np.zeros((numeroDeEstados,numeroDeAcciones))\n",
    "        self.gamma=0.5\n",
    "        \n",
    "    def Qupdate(self,estado,accion,vervose=False):\n",
    "        \"\"\"_Actualiza la función Q con la ecuación de Bellman_\n",
    "\n",
    "        Args:\n",
    "            estado (_int_): _estado actual del agente_\n",
    "            accion (_type_): _acción tomada en el estado actual_\n",
    "            vervose (bool, optional): _permite imprimit información del estado acción para saber lo que el agente esta haciendo_. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _La función Q descrita como matriz_\n",
    "        \"\"\"\n",
    "        _, recompensa, terminado, Truncado =self.env.reset() #<-- Se resetea el ambiente al estado s=3\n",
    "        self.env.current_state=estado #<-- Se cambia el estado al estado decidido fuera de la funcion \"pueders 1,2,3...\"\n",
    "        current_state_1, recompensa, terminado, Truncado=self.env.step(accion)#\"<---- Con el estado cambiado se realiza la acción a\"\n",
    "        if vervose==True:\n",
    "            print(\"Estado antes de accion\")\n",
    "            print(estado)\n",
    "            print(\"La acción tomada es !!!!!\")\n",
    "            print(accion)\n",
    "            print(\"estado actual ************\")\n",
    "            print(current_state_1)\n",
    "            print(\"Current Q value\")\n",
    "            print(self.matriz_estado_accion[current_state_1][0])\n",
    "            print(self.matriz_estado_accion[current_state_1][1])\n",
    "            print(\"Current max value\")\n",
    "            print(max(self.matriz_estado_accion[current_state_1][0],self.matriz_estado_accion[current_state_1][1]  ) )\n",
    "            print(\"Recompensa!!!!!!!!!\")\n",
    "            print(recompensa)\n",
    "        \n",
    "        Q_estocastico=0.0# Iniciamos el estado estocastico varia de 0-5\n",
    "        self.env.current_state=estado #Como se realizo una transición en la aprte superior para provar funcionamiento de .step() se reinicia el estado actual al \"estado\"\n",
    "        for estado_estocastico in range(self.env.numeroDeEstados):#--> Inicio del ciclo for\n",
    "            filaDeDinamicaEstadoAccion=estado+3*accion+3 #--> Dertemina la fila adecuada de la matriz self.Dinamica_delrobot_limpiador_estocastico\n",
    "            recompensa=self.env.reward(estado_estocastico)#--> Aquí la recomensa depende del estado actual y la intención de estado el \"estado estocastico\" y NO del estadp acción\n",
    "            Probabilidad_ocurrencia=self.env.Dinamica_delrobot_limpiador_estocastico[filaDeDinamicaEstadoAccion][estado_estocastico] #--->Determina la probabilidad de ocurrencia\n",
    "            Q_estocastico+=Probabilidad_ocurrencia*(recompensa+self.gamma*max(self.matriz_estado_accion_estocastico[estado_estocastico][0],self.matriz_estado_accion_estocastico[estado_estocastico][1]) )#Determina la actualización del valor Q \n",
    "            accion_to_index=int(0.5*accion+0.5)#Encuentra el index al que correponde colocar el valor actualizado de Q\n",
    "            self.matriz_estado_accion_estocastico[estado][accion_to_index]=Q_estocastico#Actualizamos \"self.matriz_estado_accion_estocastico\"\n",
    "        \n",
    "        return self.matriz_estado_accion_estocastico\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a46f130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-iteratio\n",
      "0\n",
      "[[0.         0.        ]\n",
      " [0.86       0.1145    ]\n",
      " [0.3698     0.049235  ]\n",
      " [0.159014   0.02117105]\n",
      " [0.31837602 4.02785355]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "1\n",
      "[[0.         0.        ]\n",
      " [0.869245   0.26311338]\n",
      " [0.3777507  0.11366803]\n",
      " [0.26312914 1.64031987]\n",
      " [1.20821697 4.13162427]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "2\n",
      "[[0.         0.        ]\n",
      " [0.86944377 0.26630856]\n",
      " [0.41486882 0.7089792 ]\n",
      " [0.50990628 1.70861716]\n",
      " [1.24331868 4.13596433]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "3\n",
      "[[0.         0.        ]\n",
      " [0.87772448 0.39942102]\n",
      " [0.44697866 0.73891338]\n",
      " [0.52711075 1.71239187]\n",
      " [1.24515407 4.13619635]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "4\n",
      "[[0.         0.        ]\n",
      " [0.87847283 0.41145081]\n",
      " [0.44961743 0.74063988]\n",
      " [0.52809025 1.71260131]\n",
      " [1.24525525 4.13620918]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "5\n",
      "[[0.         0.        ]\n",
      " [0.878516   0.41214465]\n",
      " [0.44976942 0.74073613]\n",
      " [0.52814478 1.71261293]\n",
      " [1.24526086 4.13620989]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "6\n",
      "[[0.         0.        ]\n",
      " [0.8785184  0.41218333]\n",
      " [0.44977789 0.74074148]\n",
      " [0.52814781 1.71261358]\n",
      " [1.24526117 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "7\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218548]\n",
      " [0.44977836 0.74074177]\n",
      " [0.52814798 1.71261361]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "8\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.4121856 ]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814798 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "9\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "10\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "11\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "12\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "13\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "14\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "15\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "16\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "17\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "18\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "19\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "20\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n",
      "Q-iteratio\n",
      "21\n",
      "[[0.         0.        ]\n",
      " [0.87851854 0.41218561]\n",
      " [0.44977839 0.74074179]\n",
      " [0.52814799 1.71261362]\n",
      " [1.24526119 4.13620993]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Este codigo contiene la iteración principal \n",
    "\"\"\"\n",
    "env=enviroment()\n",
    "Q=Qiteration(env,env.numeroDeEstados,env.numeroDeAcciones)\n",
    "\n",
    "\n",
    "for k in range(22):\n",
    "    for i in range(6):\n",
    "        for j in range(2):\n",
    "            estado=i\n",
    "            accion=env.acciones[j]\n",
    "            Q_state_action_matriz=Q.Qupdate(estado,accion,vervose=False)\n",
    "    print(\"Q-iteratio\")\n",
    "    print(k)\n",
    "    print(Q_state_action_matriz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb1d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2bbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
