{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2697d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class enviroment():\n",
    "    def __init__(self):\n",
    "        \"\"\"Esta clase cre el ambiente del robot movil \n",
    "\n",
    "        * self.numeroDeEstados -> Numero de estados\n",
    "        * self.acciones -> Acciones permitidas por el robot\n",
    "        * self.numeroDeAcciones -> Numero de acciones permitidas por el robot \n",
    "\n",
    "        \"\"\"\n",
    "        self.numeroDeEstados=6 #Numero de estados para el problema del robot\n",
    "        self.acciones=[-1,1] #Accciones del problema del robot\n",
    "        self.numeroDeAcciones=len(self.acciones) #Numero de acciones\n",
    "    def reset(self):\n",
    "        \"\"\"Esta función resetea el robot a la posición inicial, se recomienda utilizar justo depues de inicializar el ambiente \n",
    "            ejemplo enviroment=enviroment() --> inicia el ambiente\n",
    "        *self.current_state: 3, Estado actual del robot, al utiizar reset el estado del robot regresa a 3 independientemente de donde este\n",
    "                                por ejemplo enviroment.reset() --> resetea al estado 3\n",
    "        * terminado: False, indica que un episodio ha terminado\n",
    "        * truncate: False, indica que el agente ha llegado a un punto en donde no puede avanzar y hay que terminar el estado el espisodio \n",
    "        \"\"\"\n",
    "        self.current_state=3\n",
    "        recompensa=0\n",
    "        self.terminado=False\n",
    "        self.Truncado=False\n",
    "        self.informacion=\"El robot ha empezado\"\n",
    "        return self.current_state, recompensa, self.terminado, self.Truncado \n",
    "    def step(self,action: int):\n",
    "        \"\"\"Este metodo contiene las dinamicas del ambiente\n",
    "\n",
    "        Args:\n",
    "            action (int): contiene la acción del robot, puede ser -1 o 1\n",
    "\n",
    "        Returns:\n",
    "            _int, float, bool, bool_: Estado actual, recompensa, si termino, o quedo estancado \n",
    "        \"\"\"\n",
    "        #RECOMPENSA------------------------------------------\n",
    "        recompensa=self.reward(action)\n",
    "        #----------------------------------------------------\n",
    "        #CAMBIO DE ESTADO------------------------------------\n",
    "        if 1<=self.current_state  and self.current_state<=4:\n",
    "            self.current_state+=action\n",
    "        return self.current_state, recompensa, self.terminado, self.Truncado \n",
    "\n",
    "    def reward(self,action):\n",
    "        \"\"\"Genera la recompensa del agente al tomar la acción a_i en un estado s_i\n",
    "\n",
    "        Args:\n",
    "            action (_lista_): [-1,1] \n",
    "\n",
    "        Returns:\n",
    "            _float_: _retorna la recomepensa_\n",
    "        \"\"\"\n",
    "        #RECOMPENSA------------------------------------------\n",
    "        recompensa=0\n",
    "        if self.current_state==4 and action==1:\n",
    "            recompensa=5\n",
    "            self.terminado=True\n",
    "        if self.current_state==1 and action==-1:\n",
    "            recompensa=1\n",
    "            self.Truncado=True\n",
    "        #----------------------------------------------------\n",
    "        return recompensa\n",
    "\n",
    "class Qiteration():\n",
    "    \"\"\"_Se encarga de realizar la iteración Q para el caso deterministico del robot limpiador_\n",
    "    \"\"\"\n",
    "    def __init__(self,env,numeroDeEstados: int,numeroDeAcciones: int):\n",
    "        \"\"\"_Inicializa la iteración Q_\n",
    "\n",
    "        Args:\n",
    "            env (_class_): _Ambiente con las dinamicas de Transición _\n",
    "            numeroDeEstados (int): _Numero de estados_\n",
    "            numeroDeAcciones (int): _Numero de acciones_\n",
    "        \"\"\"\n",
    "        self.env=env\n",
    "        self.matriz_estado_accion=np.zeros((numeroDeEstados,numeroDeAcciones))\n",
    "        self.gamma=0.5\n",
    "        \n",
    "    def Qupdate(self,estado,accion,vervose=False):\n",
    "        \"\"\"_Actualiza la función Q con la ecuación de Bellman_\n",
    "\n",
    "        Args:\n",
    "            estado (_int_): _estado actual del agente_\n",
    "            accion (_type_): _acción tomada en el estado actual_\n",
    "            vervose (bool, optional): _permite imprimit información del estado acción para saber lo que el agente esta haciendo_. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _La función Q descrita como matriz_\n",
    "        \"\"\"\n",
    "        _, recompensa, terminado, Truncado =self.env.reset() #<-- Se resetea el ambiente al estado s=3\n",
    "        self.env.current_state=estado #<-- Se cambia el estado al estado decidido fuera de la funcion \"pueders 1,2,3...\"\n",
    "        current_state_1, recompensa, terminado, Truncado=self.env.step(accion)#\"<---- Con el estado cambiado se realiza la acción a\"\n",
    "        if vervose==True:\n",
    "            print(\"Estado antes de accion\")\n",
    "            print(estado)\n",
    "            print(\"La acción tomada es !!!!!\")\n",
    "            print(accion)\n",
    "            print(\"estado actual ************\")\n",
    "            print(current_state_1)\n",
    "            print(\"Current Q value\")\n",
    "            print(self.matriz_estado_accion[current_state_1][0])\n",
    "            print(self.matriz_estado_accion[current_state_1][1])\n",
    "            print(\"Current max value\")\n",
    "            print(max(self.matriz_estado_accion[current_state_1][0],self.matriz_estado_accion[current_state_1][1]  ) )\n",
    "            print(\"Recompensa!!!!!!!!!\")\n",
    "            print(recompensa)\n",
    "        #Basicamente se esta aplicando la función de tranferencia T, en el setado \"s\" y la acción \"a\"  T(s,a)\n",
    "        # A hora esto nos lleva a un nuevo estado s+1 en el cual nos hacemos la pregunta ¿Cual acción es más beneficiosa de acuerdo con mi politica?\n",
    "        # Dicho de otro modo, tomamos  el producto de Gamma por el valor esperado de tomar la mejor acción en el estado s+1\n",
    "        # Con esto se logea transmitir parate del valor esperado en Q(s+1,a')--> Q(s_i,a_i) lo que le da una metrica al estado s_i,a_i .   \n",
    "        Q=recompensa+self.gamma*max(self.matriz_estado_accion[current_state_1][0], self.matriz_estado_accion[current_state_1][1]  )\n",
    "        \n",
    "        accion_to_index=int(0.5*accion+0.5)\n",
    "        self.matriz_estado_accion[estado][accion_to_index]=Q #Aquí actualizamos es valor Q del estado acción  actual.\n",
    "        return self.matriz_estado_accion\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a46f130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-iteratio\n",
      "0\n",
      "[[0.    0.   ]\n",
      " [1.    0.   ]\n",
      " [0.5   0.   ]\n",
      " [0.25  0.   ]\n",
      " [0.125 5.   ]\n",
      " [0.    0.   ]]\n",
      "Q-iteratio\n",
      "1\n",
      "[[0.    0.   ]\n",
      " [1.    0.25 ]\n",
      " [0.5   0.125]\n",
      " [0.25  2.5  ]\n",
      " [1.25  5.   ]\n",
      " [0.    0.   ]]\n",
      "Q-iteratio\n",
      "2\n",
      "[[0.    0.   ]\n",
      " [1.    0.25 ]\n",
      " [0.5   1.25 ]\n",
      " [0.625 2.5  ]\n",
      " [1.25  5.   ]\n",
      " [0.    0.   ]]\n",
      "Q-iteratio\n",
      "3\n",
      "[[0.    0.   ]\n",
      " [1.    0.625]\n",
      " [0.5   1.25 ]\n",
      " [0.625 2.5  ]\n",
      " [1.25  5.   ]\n",
      " [0.    0.   ]]\n",
      "Q-iteratio\n",
      "4\n",
      "[[0.    0.   ]\n",
      " [1.    0.625]\n",
      " [0.5   1.25 ]\n",
      " [0.625 2.5  ]\n",
      " [1.25  5.   ]\n",
      " [0.    0.   ]]\n",
      "Q-iteratio\n",
      "5\n",
      "[[0.    0.   ]\n",
      " [1.    0.625]\n",
      " [0.5   1.25 ]\n",
      " [0.625 2.5  ]\n",
      " [1.25  5.   ]\n",
      " [0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Este codigo contiene la iteración principal \n",
    "\"\"\"\n",
    "env=enviroment()\n",
    "Q=Qiteration(env,env.numeroDeEstados,env.numeroDeAcciones)\n",
    "\n",
    "\n",
    "for k in range(6):\n",
    "    for i in range(6):\n",
    "        for j in range(2):\n",
    "            estado=i\n",
    "            accion=env.acciones[j]\n",
    "            Q_state_action_matriz=Q.Qupdate(estado,accion,vervose=False)\n",
    "    print(\"Q-iteratio\")\n",
    "    print(k)\n",
    "    print(Q_state_action_matriz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb1d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2bbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
